{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b13c5a53-bb49-41d2-9446-0283b6c786da",
   "metadata": {},
   "source": [
    "Lab Assignment 2: Data Reading and Processing\n",
    "Name: Diego T. \n",
    "Course: CS 456\n",
    "Date: Oct, 13, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665fbd7-d5b1-41d8-94d0-167d4db3456f",
   "metadata": {},
   "source": [
    "Introduction: \n",
    "This lab focuses on reading, cleaning, and combining multiple data files using Python and Pandas. The provided datasets are partially \"damaged,\" containing missing values and formatting issues that must be identified and corrected. The main goal is to integrate all files into a single, well-structured dataset and analyze it to find useful insights, such as total data volume, missing data, and company performance from 1995 to 1998. This project also emphasizes clear documentation and organized code to ensure the data processing steps are easy to follow and reproducible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7034cfab-0ff1-4000-9d33-9188dbaad59e",
   "metadata": {},
   "source": [
    "We begin by importing libraries that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75a415a-601a-414d-87fb-477d78236cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas for working with data tables\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy for handling missing or numeric values\n",
    "import numpy as np\n",
    "\n",
    "# import re for text pattern matching (used later for text file cleanup)\n",
    "import re\n",
    "\n",
    "# json for safely loading JSON objects\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03073f67-e1ad-4020-bdd2-4c0a4fcc2424",
   "metadata": {},
   "source": [
    "Step 1 - Reading and Combining the Data\n",
    "In this section, all three data files are imported and combined into one dataset. Each file uses a different format (CSV, JSON, and text), so different loading methods are used. After reading the data, all column names are standardized and merged into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b77ecf-0532-41e4-89cd-4628227724a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: (28851, 5)\n",
      "JSON shape: (10874, 5)\n",
      "TXT shape: (7600, 5)\n",
      "Combined shape: (47325, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Year</th>\n",
       "      <th>(1)Rank</th>\n",
       "      <th>!Company</th>\n",
       "      <th>(3)Revenue (in millions)</th>\n",
       "      <th>okjb)Profit (in millions)</th>\n",
       "      <th>Year</th>\n",
       "      <th>Rank</th>\n",
       "      <th>Company</th>\n",
       "      <th>Revenue (in millions)</th>\n",
       "      <th>Profit (in millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1955.0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Motors</td>\n",
       "      <td>9823.5</td>\n",
       "      <td>806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1955.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Exxon Mobil</td>\n",
       "      <td>5661.4</td>\n",
       "      <td>584.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1955.0</td>\n",
       "      <td>3</td>\n",
       "      <td>U.S. Steel</td>\n",
       "      <td>3250.4</td>\n",
       "      <td>195.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1955.0</td>\n",
       "      <td>4</td>\n",
       "      <td>General Electric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1955.0</td>\n",
       "      <td>5</td>\n",
       "      <td>Esmark</td>\n",
       "      <td>2510.8</td>\n",
       "      <td>19.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    #Year (1)Rank          !Company (3)Revenue (in millions)  \\\n",
       "0  1955.0       1    General Motors                   9823.5   \n",
       "1  1955.0       2       Exxon Mobil                   5661.4   \n",
       "2  1955.0       3        U.S. Steel                   3250.4   \n",
       "3  1955.0       4  General Electric                      NaN   \n",
       "4  1955.0       5            Esmark                   2510.8   \n",
       "\n",
       "  okjb)Profit (in millions) Year Rank Company Revenue (in millions)  \\\n",
       "0                       806  NaN  NaN     NaN                   NaN   \n",
       "1                     584.8  NaN  NaN     NaN                   NaN   \n",
       "2                     195.4  NaN  NaN     NaN                   NaN   \n",
       "3                     212.6  NaN  NaN     NaN                   NaN   \n",
       "4                      19.1  NaN  NaN     NaN                   NaN   \n",
       "\n",
       "  Profit (in millions)  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the CSV file that contains Fortune 500 data\n",
    "# some rows might have extra commas, so we skip those instead of crashing\n",
    "csv_data = pd.read_csv(\"fortune500.csv\", on_bad_lines='skip', engine='python')\n",
    "print(\"CSV shape:\", csv_data.shape)\n",
    "\n",
    "# read the JSON file line by line since each line is one JSON object\n",
    "json_records = []   # will store each valid JSON record we find\n",
    "with open(\"lines.json\", \"r\") as jfile:\n",
    "    for line in jfile:\n",
    "        line = line.strip()        # remove spaces and newline characters\n",
    "        if not line:               # skip empty lines to avoid errors\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(line)          # convert line into a dictionary\n",
    "            json_records.append(data)        # add valid line to the list\n",
    "        except json.JSONDecodeError:\n",
    "            continue                         # skip bad or incomplete lines\n",
    "\n",
    "json_data = pd.DataFrame(json_records)       # convert list into a DataFrame\n",
    "print(\"JSON shape:\", json_data.shape)\n",
    "\n",
    "# read the unstructured text file and extract key-value pairs manually\n",
    "records = []      # list for all records\n",
    "temp = {}         # temporary dictionary for one record\n",
    "with open(\"unstructureddata.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()                  # remove spaces and newlines\n",
    "        if not line:                         # blank line marks end of one record\n",
    "            if temp:                         # only add if dictionary not empty\n",
    "                records.append(temp)\n",
    "                temp = {}                    # reset for next record\n",
    "        else:\n",
    "            if \":\" in line:                  # split line if it has a key and value\n",
    "                key, value = line.split(\":\", 1)\n",
    "                temp[key.strip()] = value.strip()\n",
    "    if temp:                                 # add last record if needed\n",
    "        records.append(temp)\n",
    "\n",
    "txt_data = pd.DataFrame(records)             # make DataFrame from text records\n",
    "print(\"TXT shape:\", txt_data.shape)\n",
    "\n",
    "# combine all three datasets into one large DataFrame\n",
    "combined = pd.concat([csv_data, json_data, txt_data], ignore_index=True)\n",
    "print(\"Combined shape:\", combined.shape)\n",
    "\n",
    "# preview the first few rows to confirm that data loaded correctly\n",
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c77800-3b11-4769-a9a0-c8dbceb9eb7a",
   "metadata": {},
   "source": [
    "Step 2 - Cleaning the Data\n",
    "Here I fixed column names, removed duplicates, and made sure numbers were in the right format. I also checked for missing values and saved the cleaned version as a new CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110423a6-4b31-429b-819d-c11c62cbe803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original column names:\n",
      "['#Year', '(1)Rank', '!Company', '(3)Revenue (in millions)', 'okjb)Profit (in millions)', 'Year', 'Rank', 'Company', 'Revenue (in millions)', 'Profit (in millions)']\n",
      "\n",
      "Cleaned column names:\n",
      "['#year', '(1)rank', '!company', '(3)revenue_', 'okjb)profit_', 'year', 'rank', 'company', 'revenue_', 'profit_']\n",
      "\n",
      "Missing values per column:\n",
      "#year           18471\n",
      "(1)rank         18471\n",
      "!company        18471\n",
      "(3)revenue_     18471\n",
      "okjb)profit_    18471\n",
      "year                0\n",
      "rank               11\n",
      "company             0\n",
      "revenue_         2004\n",
      "profit_           744\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset shape: (18471, 10)\n",
      "Cleaned dataset saved as clean_combined.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f8/g51pfbbs31x4tnfh4q3bbp9c0000gn/T/ipykernel_28367/4075144424.py:38: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  combined = combined.fillna(value=np.nan)\n"
     ]
    }
   ],
   "source": [
    "# start by checking what columns exist and how they look\n",
    "print(\"Original column names:\")\n",
    "print(list(combined.columns))\n",
    "print()\n",
    "\n",
    "# make all column names lowercase, remove spaces, and replace them with underscores\n",
    "combined.columns = [\n",
    "    c.strip().lower().replace(\" \", \"_\").replace(\"(in_millions)\", \"\")\n",
    "    for c in combined.columns\n",
    "]\n",
    "\n",
    "# check again to confirm column names were cleaned correctly\n",
    "print(\"Cleaned column names:\")\n",
    "print(list(combined.columns))\n",
    "print()\n",
    "\n",
    "# remove any duplicate rows that might exist after merging multiple files\n",
    "combined = combined.drop_duplicates()\n",
    "\n",
    "# loop through columns and convert numeric-type columns to numbers when possible\n",
    "for name in combined.columns:\n",
    "    if any(word in name for word in [\"year\", \"rank\", \"revenue\", \"profit\"]):\n",
    "        # convert non-numeric values to NaN instead of causing an error\n",
    "        combined[name] = pd.to_numeric(combined[name], errors=\"coerce\")\n",
    "\n",
    "# replace blank strings in company names with NaN (so they can be dropped)\n",
    "combined[\"company\"] = combined[\"company\"].replace(\"\", np.nan)\n",
    "\n",
    "# drop any rows where the company name is missing (since it's the main identifier)\n",
    "combined = combined.dropna(subset=[\"company\"])\n",
    "\n",
    "# print total missing values per column so we know data quality\n",
    "print(\"Missing values per column:\")\n",
    "print(combined.isnull().sum())\n",
    "print()\n",
    "\n",
    "# fill obvious missing numeric values with NaN (no change, but ensures consistency)\n",
    "combined = combined.fillna(value=np.nan)\n",
    "\n",
    "# display dataset shape after cleaning\n",
    "print(\"Cleaned dataset shape:\", combined.shape)\n",
    "\n",
    "# save the cleaned dataset to a new CSV file\n",
    "combined.to_csv(\"clean_combined.csv\", index=False)\n",
    "\n",
    "# confirmation message so we know the save worked\n",
    "print(\"Cleaned dataset saved as clean_combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98988b06-6909-49d7-8340-7d94c131b264",
   "metadata": {},
   "source": [
    "Step 3 - Analyzing the Data\n",
    "Once everything is clean, I calculated a few key stats: total number of records, total missing data points, how many unique companies, which companies made the most revenue and profit between 1995-1998.\n",
    "All the results are stored in a small summary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b41496e7-6769-484e-aec9-19bcc038f127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected revenue column: (3)revenue_\n",
      "Detected profit column: okjb)profit_\n",
      "\n",
      "Summary of results:\n",
      "   total_records  missing_cells  unique_companies top_revenue_1995_1998  \\\n",
      "0          18471          95114              2226                   N/A   \n",
      "\n",
      "  top_profit_1995_1998  \n",
      "0                  N/A  \n",
      "\n",
      "Results_Combine.csv has been created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# find total number of rows in the dataset\n",
    "total_records = len(combined)\n",
    "\n",
    "# count total number of missing cells across the entire dataset\n",
    "missing_cells = combined.isnull().sum().sum()\n",
    "\n",
    "# count how many unique companies exist\n",
    "unique_companies = combined[\"company\"].nunique()\n",
    "\n",
    "# look for columns that might represent revenue or profit\n",
    "revenue_cols = [col for col in combined.columns if isinstance(col, str) and (\"revenue\" in col.lower() or \"sales\" in col.lower())]\n",
    "profit_cols = [col for col in combined.columns if isinstance(col, str) and (\"profit\" in col.lower() or \"income\" in col.lower())]\n",
    "\n",
    "# make sure we found at least one valid column name for each\n",
    "revenue_col = revenue_cols[0] if revenue_cols else None\n",
    "profit_col = profit_cols[0] if profit_cols else None\n",
    "\n",
    "# print detected column names so we can confirm visually\n",
    "print(\"Detected revenue column:\", revenue_col)\n",
    "print(\"Detected profit column:\", profit_col)\n",
    "print()\n",
    "\n",
    "# make a subset for years between 1995 and 1998 if the 'year' column exists\n",
    "if \"year\" in combined.columns:\n",
    "    subset = combined[(combined[\"year\"] >= 1995) & (combined[\"year\"] <= 1998)]\n",
    "else:\n",
    "    subset = combined.copy()  # fallback if 'year' is missing\n",
    "\n",
    "# initialize placeholder results in case we can't compute them\n",
    "top_revenue_company = \"N/A\"\n",
    "top_profit_company = \"N/A\"\n",
    "\n",
    "# only run if we found both columns and the subset isn't empty\n",
    "if not subset.empty and revenue_col and profit_col:\n",
    "    # drop rows where revenue or profit are NaN before finding max\n",
    "    subset_valid = subset.dropna(subset=[revenue_col, profit_col], how='any')\n",
    "    \n",
    "    if not subset_valid.empty:\n",
    "        # safely get top revenue company\n",
    "        try:\n",
    "            top_revenue_company = subset_valid.loc[subset_valid[revenue_col].idxmax(), \"company\"]\n",
    "        except Exception:\n",
    "            top_revenue_company = \"N/A\"\n",
    "        # safely get top profit company\n",
    "        try:\n",
    "            top_profit_company = subset_valid.loc[subset_valid[profit_col].idxmax(), \"company\"]\n",
    "        except Exception:\n",
    "            top_profit_company = \"N/A\"\n",
    "\n",
    "# create a dictionary to hold all summary results\n",
    "results_dict = {\n",
    "    \"total_records\": [total_records],\n",
    "    \"missing_cells\": [missing_cells],\n",
    "    \"unique_companies\": [unique_companies],\n",
    "    \"top_revenue_1995_1998\": [top_revenue_company],\n",
    "    \"top_profit_1995_1998\": [top_profit_company]\n",
    "}\n",
    "\n",
    "# make a new DataFrame to display results nicely\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# show results table in the output\n",
    "print(\"Summary of results:\")\n",
    "print(results_df)\n",
    "print()\n",
    "\n",
    "# save the summary to a CSV file\n",
    "results_df.to_csv(\"Results_Combine.csv\", index=False)\n",
    "\n",
    "# print confirmation message\n",
    "print(\"Results_Combine.csv has been created and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601d585e-2f5e-4bd3-928c-6f7607b1c063",
   "metadata": {},
   "source": [
    "After cleaning and merging all three datasets, the final combined file contains consistent company information across multiple years. The analysis produced the total record count, number of missing values, and identified the top companies by revenue and profit between 1995 and 1998. The cleaned dataset ('clean_combined.csv') and the analysis results ('Results_Combine.csv') were both saved successfully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
